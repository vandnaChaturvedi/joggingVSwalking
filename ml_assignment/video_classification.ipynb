{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for data\n",
    "walking_path = '/home/vandna/ml_assignment/Walking/'\n",
    "jogging_path = '/home/vandna/ml_assignment/jogging/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying the video names in respecvtive file\n",
    "walking = !ls /home/vandna/ml_assignment/Walking/\n",
    "jogging = !ls /home/vandna/ml_assignment/jogging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a numpy array for each videos of walking\n",
    "import numpy as np\n",
    "import cv2\n",
    "videos = []\n",
    "for i in walking:\n",
    "    cap = cv2.VideoCapture(walking_path+i)\n",
    "    images=[]\n",
    "    while(cap.isOpened()):\n",
    "        try:\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            edge_gray = cv2.Canny(gray,100,200)\n",
    "            images.append(edge_gray)\n",
    "            cv2.imshow('frame',edge_gray)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "    videos.append(np.array(images))\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickling of videos variable\n",
    "import pickle\n",
    "videos_walking = np.array(videos)\n",
    "file = open('videos_walking.pkl', 'wb')\n",
    "pickle.dump(videos_walking, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a numpy array for each videos of jogging\n",
    "\n",
    "videos = []\n",
    "for i in jogging:\n",
    "    cap = cv2.VideoCapture(jogging_path+i)\n",
    "    images=[]\n",
    "    while(cap.isOpened()):\n",
    "        try:\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            edge_gray = cv2.Canny(gray,100,200)\n",
    "            images.append(edge_gray)\n",
    "            cv2.imshow('frame',edge_gray)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "    videos.append(np.array(images))\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jogging  video_classification.ipynb  videos_walking_pickeled  Walking\r\n"
     ]
    }
   ],
   "source": [
    "#pickling of jog videos variable\n",
    "videos_jogging = np.array(videos)\n",
    "file = open('videos_jogging,pkl', 'wb')\n",
    "pickle.dump(videos_jogging, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading jog data\n",
    "file1 = open('videos_jogging.pkl', 'rb')\n",
    "data = pickle.load(file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cropping jog data \n",
    "sliced_videos=[]\n",
    "for i in range(len(data)):\n",
    "    sliced_videos.append(data[i][:302]) # why 302? B'coz smallest frame count in all videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#videos_jogging_cropped = np.array(sliced_videos)\n",
    "file1 = open('videos_jogging_cropped.pkl', 'wb')\n",
    "pickle.dump(sliced_videos, file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading walk data\n",
    "file1 = open('videos_walking.pkl', 'rb')\n",
    "data = pickle.load(file1)\n",
    "file1.close()\n",
    "#cropping jog data \n",
    "sliced_videos=[]\n",
    "for i in range(len(data)):\n",
    "    sliced_videos.append(data[i][:302])\n",
    "#videos_jogging_cropped = np.array(sliced_videos)\n",
    "file1 = open('videos_walking_cropped.pkl', 'wb')\n",
    "pickle.dump(sliced_videos, file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading j_cropped\n",
    "\n",
    "file1 = open('videos_jogging_cropped.pkl', 'rb')\n",
    "data1 = pickle.load(file1)#49\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading w_cropped\n",
    "file1 = open('videos_walking_cropped.pkl', 'rb')\n",
    "data = pickle.load(file1)#51\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core code operate the Knn classifier \n",
    "#only image based (Knn) classifier gives >78% Accuracy.\n",
    "#system needed 16GB RAM to run this Classfier.\n",
    "flattened_data=[]\n",
    "for i in range(len(data)):\n",
    "        for j in range(302):\n",
    "                flattened_data.append(data[i][j].flatten())\n",
    "for i in range(len(data1)):\n",
    "        for j in range(302):\n",
    "                flattened_data.append(data1[i][j].flatten())\n",
    "\n",
    "\n",
    "y=[1 for i in range(51*302)]+[0 for i in range(49*302)]\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(flattened_data, y) \n",
    "print neigh.predict(flattened_data)\n",
    "print 'labels:',y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
